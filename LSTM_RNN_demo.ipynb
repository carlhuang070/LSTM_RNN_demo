{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----2-----\n",
      "[[  2.42358601e-11  -2.81444639e-11]\n",
      " [ -2.81437618e-11   3.26826069e-11]]\n",
      "[[  2.42358601e-11  -2.81444639e-11]\n",
      " [ -2.81437618e-11   3.26826069e-11]]\n",
      "-----1-----\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[  2.42358601e-11  -2.81444639e-11]\n",
      " [ -2.81437618e-11   3.26826069e-11]]\n",
      "weights(0,0): expected - actural 2.4209e-11 - 2.4236e-11\n",
      "weights(0,1): expected - actural -2.8151e-11 - -2.8144e-11\n",
      "weights(1,0): expected - actural -2.8204e-11 - -2.8144e-11\n",
      "weights(1,1): expected - actural 3.2645e-11 - 3.2683e-11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class IdentityActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        return weighted_input\n",
    "    def backward(self, output):\n",
    "        return 1\n",
    "    \n",
    "class SigmoidActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        return 1.0 / (1.0 + np.exp(-weighted_input))\n",
    "    def backward(self, output):\n",
    "        return output * (1 - output)\n",
    "class TanhActivator(object):\n",
    "    def forward(self, weighted_input):\n",
    "        return 2.0 / (1.0 + np.exp(-2 * weighted_input)) - 1.0\n",
    "    def backward(self, output):\n",
    "        return 1 - output * output\n",
    "    \n",
    "    \n",
    "class LstmLayer(object):\n",
    "    def __init__(self, input_width, state_width, \n",
    "                 learning_rate):\n",
    "        self.input_width = input_width\n",
    "        self.state_width = state_width\n",
    "        self.learning_rate = learning_rate\n",
    "        # 门的激活函数\n",
    "        self.gate_activator = SigmoidActivator()\n",
    "        # 输出的激活函数\n",
    "        self.output_activator = TanhActivator()\n",
    "        # 当前时刻初始化为t0\n",
    "        self.times = 0       \n",
    "        # 各个时刻的单元状态向量c\n",
    "        self.c_list = self.init_state_vec()\n",
    "        # 各个时刻的输出向量h\n",
    "        self.h_list = self.init_state_vec()\n",
    "        # 各个时刻的遗忘门f\n",
    "        self.f_list = self.init_state_vec()\n",
    "        # 各个时刻的输入门i\n",
    "        self.i_list = self.init_state_vec()\n",
    "        # 各个时刻的输出门o\n",
    "        self.o_list = self.init_state_vec()\n",
    "        # 各个时刻的即时状态c~\n",
    "        self.ct_list = self.init_state_vec()\n",
    "        # 遗忘门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wfh, self.Wfx, self.bf = (\n",
    "            self.init_weight_mat())\n",
    "        # 输入门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wih, self.Wix, self.bi = (\n",
    "            self.init_weight_mat())\n",
    "        # 输出门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Woh, self.Wox, self.bo = (\n",
    "            self.init_weight_mat())\n",
    "        # 单元状态权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wch, self.Wcx, self.bc = (\n",
    "            self.init_weight_mat())\n",
    "            \n",
    "        \n",
    "    def init_state_vec(self):\n",
    "        '''\n",
    "        初始化保存状态的向量\n",
    "        '''\n",
    "        state_vec_list = []\n",
    "        state_vec_list.append(np.zeros(\n",
    "            (self.state_width, 1)))\n",
    "        return state_vec_list\n",
    "    def init_weight_mat(self):\n",
    "        '''\n",
    "        初始化权重矩阵\n",
    "        '''\n",
    "        Wh = np.random.uniform(-1e-4, 1e-4,\n",
    "            (self.state_width, self.state_width))\n",
    "        Wx = np.random.uniform(-1e-4, 1e-4,\n",
    "            (self.state_width, self.input_width))\n",
    "        b = np.zeros((self.state_width, 1))\n",
    "        return Wh, Wx, b\n",
    "    \n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        根据式1-式6进行前向计算\n",
    "        '''\n",
    "        self.times += 1\n",
    "        # 遗忘门\n",
    "        fg = self.calc_gate(x, self.Wfx, self.Wfh, \n",
    "            self.bf, self.gate_activator)\n",
    "        self.f_list.append(fg)\n",
    "        # 输入门\n",
    "        ig = self.calc_gate(x, self.Wix, self.Wih,\n",
    "            self.bi, self.gate_activator)\n",
    "        self.i_list.append(ig)\n",
    "        # 输出门\n",
    "        og = self.calc_gate(x, self.Wox, self.Woh,\n",
    "            self.bo, self.gate_activator)\n",
    "        self.o_list.append(og)\n",
    "        # 即时状态\n",
    "        ct = self.calc_gate(x, self.Wcx, self.Wch,\n",
    "            self.bc, self.output_activator)\n",
    "        self.ct_list.append(ct)\n",
    "        # 单元状态\n",
    "        c = fg * self.c_list[self.times - 1] + ig * ct\n",
    "        self.c_list.append(c)\n",
    "        # 输出\n",
    "        h = og * self.output_activator.forward(c)\n",
    "        self.h_list.append(h)\n",
    "     \n",
    "    \n",
    "    \n",
    "    def calc_gate(self, x, Wx, Wh, b, activator):\n",
    "        '''\n",
    "        计算门\n",
    "        '''\n",
    "        h = self.h_list[self.times - 1] # 上次的LSTM输出\n",
    "        net = np.dot(Wh, h) + np.dot(Wx, x) + b\n",
    "        gate = activator.forward(net)\n",
    "        return gate\n",
    "    \n",
    "    \n",
    "\n",
    "    def backward(self, x, delta_h, activator):\n",
    "        '''\n",
    "        实现LSTM训练算法\n",
    "        '''\n",
    "        self.calc_delta(delta_h, activator)\n",
    "        self.calc_gradient(x)\n",
    "        \n",
    "        \n",
    "    def calc_delta(self, delta_h, activator):\n",
    "        # 初始化各个时刻的误差项\n",
    "        self.delta_h_list = self.init_delta()  # 输出误差项\n",
    "        self.delta_o_list = self.init_delta()  # 输出门误差项\n",
    "        self.delta_i_list = self.init_delta()  # 输入门误差项\n",
    "        self.delta_f_list = self.init_delta()  # 遗忘门误差项\n",
    "        self.delta_ct_list = self.init_delta() # 即时输出误差项\n",
    "        # 保存从上一层传递下来的当前时刻的误差项\n",
    "        self.delta_h_list[-1] = delta_h\n",
    "        # 迭代计算每个时刻的误差项\n",
    "        for k in range(self.times, 0, -1):\n",
    "            self.calc_delta_k(k)\n",
    "    def init_delta(self):\n",
    "        '''\n",
    "        初始化误差项\n",
    "        '''\n",
    "        delta_list = []\n",
    "        for i in range(self.times + 1):\n",
    "            delta_list.append(np.zeros(\n",
    "                (self.state_width, 1)))\n",
    "        return delta_list\n",
    "    \n",
    "    \n",
    "    def calc_delta_k(self, k):\n",
    "        '''\n",
    "        根据k时刻的delta_h，计算k时刻的delta_f、\n",
    "        delta_i、delta_o、delta_ct，以及k-1时刻的delta_h\n",
    "        '''\n",
    "        # 获得k时刻前向计算的值\n",
    "        ig = self.i_list[k]\n",
    "        og = self.o_list[k]\n",
    "        fg = self.f_list[k]\n",
    "        ct = self.ct_list[k]\n",
    "        c = self.c_list[k]\n",
    "        c_prev = self.c_list[k-1]\n",
    "        tanh_c = self.output_activator.forward(c)\n",
    "        delta_k = self.delta_h_list[k]\n",
    "        # 根据式9计算delta_o\n",
    "        delta_o = (delta_k * tanh_c * \n",
    "            self.gate_activator.backward(og))\n",
    "        delta_f = (delta_k * og * \n",
    "            (1 - tanh_c * tanh_c) * c_prev *\n",
    "            self.gate_activator.backward(fg))\n",
    "        delta_i = (delta_k * og * \n",
    "            (1 - tanh_c * tanh_c) * ct *\n",
    "            self.gate_activator.backward(ig))\n",
    "        delta_ct = (delta_k * og * \n",
    "            (1 - tanh_c * tanh_c) * ig *\n",
    "            self.output_activator.backward(ct))\n",
    "        delta_h_prev = (\n",
    "                np.dot(delta_o.transpose(), self.Woh) +\n",
    "                np.dot(delta_i.transpose(), self.Wih) +\n",
    "                np.dot(delta_f.transpose(), self.Wfh) +\n",
    "                np.dot(delta_ct.transpose(), self.Wch)\n",
    "            ).transpose()\n",
    "        # 保存全部delta值\n",
    "        self.delta_h_list[k-1] = delta_h_prev\n",
    "        self.delta_f_list[k] = delta_f\n",
    "        self.delta_i_list[k] = delta_i\n",
    "        self.delta_o_list[k] = delta_o\n",
    "        self.delta_ct_list[k] = delta_ct\n",
    "        \n",
    "        \n",
    "        \n",
    "    def calc_gradient(self, x):\n",
    "        # 初始化遗忘门权重梯度矩阵和偏置项\n",
    "        self.Wfh_grad, self.Wfx_grad, self.bf_grad = (\n",
    "            self.init_weight_gradient_mat())\n",
    "        # 初始化输入门权重梯度矩阵和偏置项\n",
    "        self.Wih_grad, self.Wix_grad, self.bi_grad = (\n",
    "            self.init_weight_gradient_mat())\n",
    "        # 初始化输出门权重梯度矩阵和偏置项\n",
    "        self.Woh_grad, self.Wox_grad, self.bo_grad = (\n",
    "            self.init_weight_gradient_mat())\n",
    "        # 初始化单元状态权重梯度矩阵和偏置项\n",
    "        self.Wch_grad, self.Wcx_grad, self.bc_grad = (\n",
    "            self.init_weight_gradient_mat())\n",
    "       # 计算对上一次输出h的权重梯度\n",
    "        for t in range(self.times, 0, -1):\n",
    "            # 计算各个时刻的梯度\n",
    "            (Wfh_grad, bf_grad,\n",
    "            Wih_grad, bi_grad,\n",
    "            Woh_grad, bo_grad,\n",
    "            Wch_grad, bc_grad) = (\n",
    "                self.calc_gradient_t(t))\n",
    "            # 实际梯度是各时刻梯度之和\n",
    "            self.Wfh_grad += Wfh_grad\n",
    "            self.bf_grad += bf_grad\n",
    "            self.Wih_grad += Wih_grad\n",
    "            self.bi_grad += bi_grad\n",
    "            self.Woh_grad += Woh_grad\n",
    "            self.bo_grad += bo_grad\n",
    "            self.Wch_grad += Wch_grad\n",
    "            self.bc_grad += bc_grad\n",
    "            print ('-----%d-----' % t)\n",
    "            print (Wfh_grad)\n",
    "            print (self.Wfh_grad)\n",
    "        # 计算对本次输入x的权重梯度\n",
    "        xt = x.transpose()\n",
    "        self.Wfx_grad = np.dot(self.delta_f_list[-1], xt)\n",
    "        self.Wix_grad = np.dot(self.delta_i_list[-1], xt)\n",
    "        self.Wox_grad = np.dot(self.delta_o_list[-1], xt)\n",
    "        self.Wcx_grad = np.dot(self.delta_ct_list[-1], xt)\n",
    "    def init_weight_gradient_mat(self):\n",
    "        '''\n",
    "        初始化权重矩阵\n",
    "        '''\n",
    "        Wh_grad = np.zeros((self.state_width,\n",
    "            self.state_width))\n",
    "        Wx_grad = np.zeros((self.state_width,\n",
    "            self.input_width))\n",
    "        b_grad = np.zeros((self.state_width, 1))\n",
    "        return Wh_grad, Wx_grad, b_grad\n",
    "    def calc_gradient_t(self, t):\n",
    "        '''\n",
    "        计算每个时刻t权重的梯度\n",
    "        '''\n",
    "        h_prev = self.h_list[t-1].transpose()\n",
    "        Wfh_grad = np.dot(self.delta_f_list[t], h_prev)\n",
    "        bf_grad = self.delta_f_list[t]\n",
    "        Wih_grad = np.dot(self.delta_i_list[t], h_prev)\n",
    "        bi_grad = self.delta_f_list[t]\n",
    "        Woh_grad = np.dot(self.delta_o_list[t], h_prev)\n",
    "        bo_grad = self.delta_f_list[t]\n",
    "        Wch_grad = np.dot(self.delta_ct_list[t], h_prev)\n",
    "        bc_grad = self.delta_ct_list[t]\n",
    "        return Wfh_grad, bf_grad, Wih_grad, bi_grad, \\\n",
    "               Woh_grad, bo_grad, Wch_grad, bc_grad\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        '''\n",
    "        按照梯度下降，更新权重\n",
    "        '''\n",
    "        self.Wfh -= self.learning_rate * self.Whf_grad\n",
    "        self.Wfx -= self.learning_rate * self.Whx_grad\n",
    "        self.bf -= self.learning_rate * self.bf_grad\n",
    "        self.Wih -= self.learning_rate * self.Whi_grad\n",
    "        self.Wix -= self.learning_rate * self.Whi_grad\n",
    "        self.bi -= self.learning_rate * self.bi_grad\n",
    "        self.Woh -= self.learning_rate * self.Wof_grad\n",
    "        self.Wox -= self.learning_rate * self.Wox_grad\n",
    "        self.bo -= self.learning_rate * self.bo_grad\n",
    "        self.Wch -= self.learning_rate * self.Wcf_grad\n",
    "        self.Wcx -= self.learning_rate * self.Wcx_grad\n",
    "        self.bc -= self.learning_rate * self.bc_grad\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset_state(self):\n",
    "        # 当前时刻初始化为t0\n",
    "        self.times = 0       \n",
    "        # 各个时刻的单元状态向量c\n",
    "        self.c_list = self.init_state_vec()\n",
    "        # 各个时刻的输出向量h\n",
    "        self.h_list = self.init_state_vec()\n",
    "        # 各个时刻的遗忘门f\n",
    "        self.f_list = self.init_state_vec()\n",
    "        # 各个时刻的输入门i\n",
    "        self.i_list = self.init_state_vec()\n",
    "        # 各个时刻的输出门o\n",
    "        self.o_list = self.init_state_vec()\n",
    "        # 各个时刻的即时状态c~\n",
    "        self.ct_list = self.init_state_vec()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def data_set():\n",
    "    x = [np.array([[1], [2], [3]]),\n",
    "         np.array([[2], [3], [4]])]\n",
    "    d = np.array([[1], [2]])\n",
    "    return x, d\n",
    "\n",
    "def gradient_check():\n",
    "    '''\n",
    "    梯度检查\n",
    "    '''\n",
    "    # 设计一个误差函数，取所有节点输出项之和\n",
    "    error_function = lambda o: o.sum()\n",
    "    lstm = LstmLayer(3, 2, 1e-3)\n",
    "    # 计算forward值\n",
    "    x, d = data_set()\n",
    "    lstm.forward(x[0])\n",
    "    lstm.forward(x[1])\n",
    "    # 求取sensitivity map\n",
    "    sensitivity_array = np.ones(lstm.h_list[-1].shape,\n",
    "                                dtype=np.float64)\n",
    "    # 计算梯度\n",
    "    lstm.backward(x[1], sensitivity_array, IdentityActivator())\n",
    "    # 检查梯度\n",
    "    epsilon = 10e-4\n",
    "    for i in range(lstm.Wfh.shape[0]):\n",
    "        for j in range(lstm.Wfh.shape[1]):\n",
    "            lstm.Wfh[i,j] += epsilon\n",
    "            lstm.reset_state()\n",
    "            lstm.forward(x[0])\n",
    "            lstm.forward(x[1])\n",
    "            err1 = error_function(lstm.h_list[-1])\n",
    "            lstm.Wfh[i,j] -= 2*epsilon\n",
    "            lstm.reset_state()\n",
    "            lstm.forward(x[0])\n",
    "            lstm.forward(x[1])\n",
    "            err2 = error_function(lstm.h_list[-1])\n",
    "            expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "            lstm.Wfh[i,j] += epsilon\n",
    "            print ('weights(%d,%d): expected - actural %.4e - %.4e' % (\n",
    "                i, j, expect_grad, lstm.Wfh_grad[i,j]))\n",
    "    return lstm\n",
    "            \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gradient_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
